# 毕业论文可能会用到的资料整理与感悟

[TOC]

---

## 一、 AlphaGo的制胜秘诀：蒙特卡洛树搜索初学者指南

原链接：https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/79767024 

- [x] 链接附带的井字棋源码：https://github.com/int8/monte-carlo-tree-search 


**通过这篇文章拓展出了两个文章：** （百度搜索关键词PUCT）

- [ ] https://blog.csdn.net/windowsyun/article/details/88770799


- [ ] https://blog.csdn.net/mergerly/article/details/83788862


> Alpha Go Zero 将多种方法集于一体，其核心组件包括：
>
> - **蒙特卡洛树搜索** ——包含了用于树遍历的 [PUCT](https://blog.csdn.net/windowsyun/article/details/88770799) 函数的某些变体
>
> - **残差卷积网络** ——其中的策略和价值网络在游戏中被用于棋局评估以及落子位置的先验概率估计
>
> - **强化学习**——通过自我博弈来训练网络
>

### 1. **介绍：** 

这里没有什么需要注意的内容，所以我没复制。

### 2. **蒙特卡洛树搜索的基本概念** 

#### 一段看不懂的综述：

> 蒙特卡洛树搜索的主要概念是搜索。**搜索**是一组沿着博弈树向下的遍历过程。单次遍历的路径会从根节点（当前博弈状态）开始，一直到达**未完全展开**的节点。未完全展开的节点意味着**其子节点至少有一个未被访问**。
> 
> 当遇到未完全展开的节点时，其未访问子节点中的一个会被选为**单次模拟的根节点**。然后模拟结果会被反向传播会**当前的根节点**，并更新博弈树节点的统计信息 。 一旦搜索（受时间或计算能力限制）终止，算法就会根据收集的统计信息选择行动策略。

#### 模拟（rollout）：

>在模拟过程中我们如何选择动作呢？答案就是 rollout 策略函数： 
>![img](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjyZpFicVhPiawBQqR0PQDEH8x0NicP0hpELn9kdsEibjUIKMdxH5icuiahXroWicibBcxcRg0SChvMpXicwZmw/640?wx_fmt=png) 
>该函数会根据输入的博弈状态产生下一个“移动/动作”。在实践中，这一函数的计算速度很快，从而可以进行很多次的模拟——默认的 rollout 策略函数会使用服从均匀分布的随机采样 。
>
>模拟 / rollout 的最简单形式只是一系列随机的从给定的博弈状态开始并终止的行动。模拟总会产生一个评估，就围棋而言，这个评估结果就是胜利，失败或平局，但通常模拟结果出现值都是可以的.

（默认策略，随机下棋直到游戏结束（用while循环控制），然后返回游戏结果：黑赢，白赢，平局）。这一步有代码参考，我现在有点想不起来，但是看了代码之后我有印象。而且原文中也说了两点，

第一点：**该函数会根据输入的博弈状态产生下一个“移动/动作”**。所以我们要明确一下，这个博弈状态究竟是state呢，还是棋盘的二维数组呢？答案很显然，就是state。因为棋盘无法体现出player这个信息，而state是由棋盘和player两部分构成的。

第二点：**默认的 rollout 策略函数会使用服从均匀分布的随机采样。**那我们应该怎么实现这个随机性呢？我们可以通过state里面的get_available_actions方法（本来设计的时候就要设计这个方法，因为产生子节点的时候还需要用）。这个方法就是通过棋盘上的空点，来形成“可以选择的动作组”，然后用一个列表接收，获取列表长度，然后在0到列表长度之间选出一个动作，然后通过这个动作下一颗子，然后再把这个下过子的棋盘包装成下一个state。当然，还需要做一个while循环，直到棋局分出输赢，或者平局，记录一下这个结果，然后退出循环，就行了。主要是要得到最终这个随便下棋的结果，看究竟是哪边赢了。

#### 博弈树节点展开——完全展开节点和访问节点

> 我们可以想一下人类是如何思考围棋或国际象棋博弈的。
> 给定一个根节点并加上博弈规则，博弈树的其余部分其实就已经隐含地表示出来了。我们不需要将整个树存储在内存中就可以实现对它的遍历。在初始形式中，博弈树是没有展开的。在最初的博弈状态中，我们处于博弈树的根部，其余节点都还没有被访问。一旦需要选择一个行动，我们就会想象这个行动会带来的结果（即到达哪个节点位置），并分析（评估）这个行动会到达的节点位置。而那些我们从未考虑过的博弈状态则会继续等待我们去发现。

最开始的时候，整个博弈树中只有一个根节点，没有子节点。子节点列表虽然存在，但是是空的表。一旦需要选择一个行动，才会考虑这个行动将会到达哪个子节点，并分析（评估）这个行动会到达的节点的位置。但是我们从未考虑过的博弈状态依然是不存在的，会继续等待我们发现。就是说，刚开始的根节点是不存在任何子节点的，每当我们从这个根节点选出一个行动的时候，对应的子节点才产生。

>蒙特卡洛树搜索博弈树也具有同样的特性。节点会被分为已访问节点或未访问节点。那么节点被访问代表什么呢？**这代表着该节点已经至少进行了一次评估** 。如果一个节点的所有子节点都被访问了，则该节点即会认为是完全展开的 ，否则它就是未完全展开节点，并且有可能进一步展开。

**什么叫已访问节点：**
不知道如何判断一个节点的子节点是否已经被访问 -> **<u>节点被访问，代表的是此节点至少已经进行了一次评估</u>**。而评估是什么时候进行的？是UCT函数进行评估？或者是rollout进行评估？这个评估究竟是指一个局面的UCB值，还是一个局面的胜负情况，这个并没有说清楚。（明白之后回来更新：就是rollout的结果，是哪边赢了的信息，让反向传播去往上更新，更新的时候就是更新N值和Q值。N值就是访问次数。每次传播，都会加1.只要N是大于0的，就算是已访问节点）

**如何产生子节点？** （这个我能实现）
在节点内部会定义一个“<u>可选择的行动组</u>”，和一个“<u>子节点列表</u>”。每当通过这个行动组产生一个子节点，这个相应的行动就会从行动组中消失。当需要检查**“是否还存在未探索过的子节点”**的时候，只需要看看这个行动组里面还有没有行动。如果还有行动，说明还存在未产生的子节点。如果行动组是空的，说明子节点已经完全产生了。因此，这要求<u>“可选择的行动组”必须在节点一产生的时候就初始化好，等待被挑出行动。</u>并且这要求子节点列表要配合好。<u>一个节点一产生的时候，他的子节点列表也初始化，但是仅限于初始化，此时的子节点列表是空的，长度是0。</u>然后当一个行动被挑选出来的时候，这个行动会在“可选择的行动组”中被删除，然后紧接着根据这个行动产生一个新的子节点，并把这个子节点添加到当前节点的子节点列表里。但是注意，如果在这种条件下，是否可以说：一个节点只要子节点都全部产生了，就是完全展开呢？答案是：不能。你光产生了一堆子节点，但是如果你都不访问呢？如果你虽然把全部子节点都产生了，但是你一个子节点都不访问，那根本不叫完全展开。

**是否完全展开：** 
注意这句话：**如果一个节点的所有子节点都被访问了，则该节点才会认为是完全展开的 ，否则它就是未完全展开节点，并且有可能进一步展开。**子节点不是说只要产生了就算是访问了，而是产生之后，还要至少进行一次评估，才算是被访问了一次。
那么，是不是可以说，只要评估一次，就是访问了一次呢？（明白之后回来更新：是这么回事。虽然不算完全正确，但是评估的时候确实会更新访问次数）

> 在**搜索**开始时，所有根节点的子节点都是未被访问的。算法会选中一个节点 ，然后开始第一次模拟（评估）。
> 请注意， 模拟期间由 rollout 策略函数选中的节点仍会被标记为未访问 。即使它们通过了 rollout ，也仍然是未访问的，**只有模拟开始的那个节点会被标记为已访问。** 

首先，搜索开始的时候，子节点压根就没有生成，而只会产生一个“可选择的行动组”。与其说算法是选中了一个节点，不如说是算法先在“可选择的行动组”里面随机找（这个随机的方式就是记录行动组的长度，然后在0到长度范围内生成下标，然后把这个下标对应的动作拿出来），然后找到一个动作之后，根据这个动作产生了一个子节点。

而且，这上面这段话还说，虽然选中了这个节点，但是这个点还是不算被访问了，就算是“它通过了rollout”，还是特马不算被访问了。
但是问题来了，**只有模拟开始的那个节点会被标记为已访问。**可是，被rollout选中的那个子节点不就是“模拟开始的那个节点”吗？？？？这个“模拟开始的那个节点”究竟是哪个节点？

#### 反向传播

> 一旦完成了一次新访问节点（有时称为叶节点）的模拟，其结果就将反向传播回当前的博弈树根节点。模拟开始的节点就会被标记为已访问 。

我感觉这句话说的，不够精确。总之我是没弄明白。

> 反向传播是从叶节点（模拟开始的节点）到根节点的遍历。模拟结果会被传送到根节点，并且反向传播路径上的每个节点的统计数据都会被计算/更新。反向传播了保证每个节点的统计数据将会反映在其所有后代节点所开始的模拟结果中（因为模拟结果会被传送到博弈树根节点）

反向传播的话，相对来讲算是中等难度。我会的地方是调用递归，不会的地方是，我应该传入什么，然后改变哪些Node里的属性？

刚才突然有了想法，我意识到实际上，传入的是两个参数，一个是在rollout过程中的输赢情况，另一个是需要执行反向传播的节点。这是在把backup定义成静态方法的前提下。实际上，如果定义成了成员方法，那么就不用传入节点了。直接用this表示当前对象就行了，然后递归的话，让父节点执行backup就成了，因为this节点里面肯定记录了父节点是谁。用this的parent就行了。就这样一直递归直到**父节点的父节点是null**的时候。为啥是父节点的父节点呢？首先根节点是不需要传播的，因为根节点只管选就行了。所以我们只需要递归到根节点的子节点那一步就行。而从分支往根的方向上看的话，那就是递归到父节点的父节点不是null的那一层，一旦父节点的父节点是null，那说明父节点是根节点了。而根节点是不需要反向传播的。因为根节点就是当前的局面。虽然有点绕，但是结论就是这样：**一直递归直到父节点的父节点是null的时候停止递归。** **这个结论我确认过四次了，没有问题，在写代码的时候直接用。**

那么还有一个问题：我究竟要改变哪些内容？那肯定是Q值和N值了。N值就是访问次数，Q就是赢了还是输了。因为“赢了还是输了”这个是跟当前节点层是谁在那里玩有关系，所以还需要分类看一下。如果是玩家的回合，玩家赢了，就加一分。否则减一分。电脑也是同样的道理，需要根据当前层的玩家来判断才行。具体看的方式我不会弄，但是那个python的代码里有这一步，我可以看一下。

#### 节点的统计数据

> 反向传播模拟结果的目的是更新反向传播路径上的所有节点 v（包括模拟开始的节点）的总模拟奖励 Q(v)  和总访问次数 N(v) 。
>
> **Q(v) - 总模拟奖励**是节点v 的一个属性，最简单的形式是通过考虑的节点的模拟结果的总和。
>
> **N(v) - 总访问次数**是节点v 的另一个属性，表示一个节点在反向传播路径上的次数（同时是它对总模拟奖励贡献的次数）
>
> 每个已访问节点都会保留这两个值，一旦完成了特定次数的模拟，已访问节点就会将这些代表它们如何被展开/探索的信息存储下来。
>
> 换句话说，随便查看一个节点的统计数据，这两个值都可以反映该节点的潜在价值（总模拟奖励）以及被探索的程度（总访问次数）。总模拟奖励较高的节点会是很好的候选节点，但访问量较低的节点也可能很值得访问（因为它们还没有被很好地探索 ）。 
>
> 现在我们已经知道了访问节点的含义，但还剩一条线没有完成。如何才能从根节点到达未访问节点，然后开始模拟呢？

文章中说，“现在我们已经知道了访问节点的含义”，那这段内容就解释了已访问节点是什么。已访问节点，我自己的感觉，就是N值起码要大于等于1的节点。N值就是总访问次数啊。那N值是什么时候更新的呢？在反向传播的时候更新的，this节点的N值会加1。那么，模拟（rollout）与反向传播，又是个什么关系呢？我知道反向传播肯定是在模拟之后，但是怎么才算模拟结束？答：模拟是一个while循环，只要循环出不来，就不算结束。而当循环出来的时候，就一定是胜负已分或者是平局的情况。这样，只要循环一出来，就一定是胜负已分或者平局。然后我们需要记录，究竟是电脑赢了，还是玩家赢了，还是平局了。模拟结束，break循环，然后返回并把结果放一个变量里。再给反向传播，反向传播就拿到了最终这个结果，就开始一步一步往上传。

#### 博弈树遍历

> 在搜索的一开始，由于我们还没有进行过任何模拟，所以首先要选择未访问的节点。单个模拟会从这些节点开始，结果会被反向传播到根节点，然后根节点就会被认为完全展开 。
>
> 但接下来我们该怎么做？ 如何才能从一个完全展开的节点到达未访问的节点呢？
> ![img](F:\09.2020.02.07在桌面的东西\毕业论文相关\#00.所有关于毕业论文的MarkDown文档\p-1585142688063)
> 当前节点（蓝色）是完全展开的，因此它肯定已经被访问了，并且存储了节点统计信息：总模拟奖励和总访问次数。其子节点同样也是已访问的，并且存储了节点统计信息。 这些值就可以用于树的上限置信区间（ UCT ）计算。

这段实际上说的就很清楚了已经。就是说：如果还有未访问的，优先选那些未访问的。如果都访问了，选UCB好的。现在的问题就是，怎么样才算是UCB好的。

#### 树的上限置信区间

> ![img](F:\09.2020.02.07在桌面的东西\毕业论文相关\#00.所有关于毕业论文的MarkDown文档\p-1585142903678)

现在就是UCB这块儿怎么处理，我还搞不明白。我感觉是需要分层讨论是哪边下棋。还是要继续看一下文献。

#### 终止蒙特卡洛树搜索

> 在构建一个博弈引擎时，你的“思考时间”可能是有限的，再加上你的计算能力也有其限制。因此，最稳妥的选择是只要在资源允许范围内，就可以运行 MCTS 。 一旦完成 MCTS ，最优的一步通常是总访问次数 N(v_i) 最高的节点，因为它的值是被估计的最好的（节点的自身估计值一定是很高的，并且同是也是被探索次数最多的节点）

注意一点：**一旦完成 MCTS ，最优的一步通常是总访问次数 N(v_i) 最高的节点**。之前那个python算法里的take_action我一直不明白，这部分还是需要看UCT的伪码来实现。反正最后就是选N最大的。这个选节点实际上是与UCB有关系，最好能改成重复使用的那种。

### 3. 总结

```python
def monte_carlo_tree_search(root):
    while resources_left(time, computational power):
        leaf = traverse(root) # leaf = unvisited node 
        simulation_result = rollout(leaf)
        backpropagate(leaf, simulation_result)
    return best_child(root)
def traverse(node):
    while fully_expanded(node):
        node = best_uct(node)
    return pick_univisted(node.children) or node # in case no children are present（缺席。意思是确保所有子节点都被探索） / node is terminal 
def rollout(node):
    while non_terminal(node):
        node = rollout_policy(node)
    return result(node) 
def rollout_policy(node):
    return pick_random(node.children)
def backpropagate(node, result):
   if is_root(node) return 
   node.stats = update_stats(node, result) 
   backpropagate(node.parent)
def best_child(node):
    pick child with highest number of visits
```

## 二、如何学习蒙特卡罗树搜索（MCTS）

原文章：https://zhuanlan.zhihu.com/p/30458774

链接附带的MCTS源码：https://github.com/tobegit3hub/ml_implementation/tree/master/monte_carlo_tree_search

由此文章产生的文章：http://www.algorithmdog.com/alphago-zero-notes

### 1. 简介

### 2. MCTS初探

> MCTS要解决的问题是搜索空间足够大，不能计算得到所有子树的价值，这是需要一种较为高效的搜索策略，同时也得兼顾探索和利用，避免陷入局部最优解。MCTS实现这些特性的方式有多种，例如经典的UCB（Upper Confidence  Bounds）算法，就是**在选择子节点的时候优先考虑没有探索过的，如果都探索过就根据得分来选择**，得分不仅是由这个子节点最终赢的概率来，而且与这个子节点玩的次数成负相关，也就是说这个子节点如果平均得分高就约有可能选中（因为认为它比其他节点更值得利用），同时如果子节点选中次数较多则下次不太会选中（因为其他节点选择次数少更值得探索），因此MCTS根据配置探索和利用不同的权重，可以实现比随机或者其他策略更有启发式的方法。

加粗的句子，和上篇文章一样，实际上说的就是treePolicy

这段话意思是说N值不重要。但是最终选的时候是选的N值最大的节点

### 3. Game Theory基础

> 什么是纳什均衡点呢？这是Game  theory里面很神奇的概念，就是所有人已经选择了对自己而言的最优解并且自己单方面做其他选择也无法再提高的点。也就是说，如果玩家都是高手，能达到或者逼近纳什均衡的策略就是最优策略，如果对手不是高手不会选择最优策略，那么纳什均衡点不一定保证每局都赢，但长远来看极大概率会赢这样的新手。

这段话可以留着吹逼用。

### 4. Black box optimization基础

> 对于Exploration和Exploitation，我在 [一种更好的超参数调优方式](https://zhuanlan.zhihu.com/p/29779000) 有更详细的剖析，感兴趣可以关注下 。

上次那个上海大学开源社区六子棋那个钟什么的，也提到了**超参数**这个东西。如果时间允许还想吹逼的话，我觉得需要查一下。

### 5.UCB算法基础

>前面讲了Game theory和Black box optimization，这里再补充一个UCB算法基础，这是MCTS的经典实现UCT（Upper Confidence bounds for Trees）里面用到的算法。

这句话就暴露了，UCB算法是UCT搜索中的一个算法。把他俩的层级关系暴露出来了

>算法本身很简单，公式如下。
>
>![img](https://pic3.zhimg.com/80/v2-2d346f466216d11a93431c4feb77c52a_1440w.jpg)
>
>其中v'表示当前树节点，v表示父节点，Q表示这个树节点的累计quality值，N表示这个树节点的visit次数，C是一个常量参数（可以控制exploitation和exploration权重）。

告诉了UCB的公式。实际上还是比较好实现。注意一个细节：图里说的是针对一个子节点而言的。而实际写代码的时候，是当前节点需要选出一个子节点，是针对父节点而言的。所以v实际上才代表当前节点，而v'代表的是当前节点的子节点。但是需要注意分母为0的情况。分母为0表示子节点还没有被访问。但是，究竟是什么时候，会导致子节点没被访问但是还需要计算这个子节点UCB呢？我觉得这个是需要从头到尾写一遍伪代码才能弄懂。

> 用Python也很容易实现这个算法，其中C常量我们可以使用 ![[公式]](https://www.zhihu.com/equation?tex=1+%2F+%5Csqrt%7B2%7D) ，这是Kocsis、Szepesvari提出的经验值，完整代码如下。
>
> ![preview](https://pic3.zhimg.com/v2-34dfa69432782d759622e17f3b33973e_r.jpg)

首先注意，只有在挑选子节点的时候才需要用这个公式。这个公式的目的就是为了挑出一个子节点。因此，应该是由子节点调用这个公式。

这个python的伪码说的也很清楚了：如果当前这个子节点已经被探索过了（或者访问过了，这两个概念是同一个意思，就是看N值是不是0），那么常数就不是0。如果当前这个子节点没有被探索过，那么就是0

### 6.MCTS算法原理

> MCTS的算法分为四步，第一步是Selection，就是在树中找到一个最好的值得探索的节点，一般策略是先选择未被探索的子节点，如果都探索过就选择UCB值最大的子节点。

这里考虑的还不够周全。UCB那个伪码图片里面，还考虑到了如果当前这个节点就是终端节点的情况。

如果当前这个节点就是终端节点，那就没有子节点。然后就会直接返回这个节点。并且在defaultPolicy这里面，对于终端节点的处理是直接返回局面输赢情况，而不再进行模拟了（本来就是赢了或者输了，那还接着下干啥。接着下反而会出bug，所以直接返回）。然后反向传播是需要老老实实进行的。

除了是终端节点的情况，就按照计划进行。没有完全扩展就去扩展，全都探索过就选UCB最高的。注意，这里说的是“全都探索过”，而不是“全都展开过”。展开很容易，产生一个子节点就算是展开了。但是探索过可不一样。探索过是指被反向传播过了。因为只有反向传播的时候，N值才会加。只有N值大于0，才是至少探索了一次，或者叫至少访问了一次。

但是，如果都探索过，是不是一定要选择UCB最大的呢？不同层的局面是不同的玩家，那按照极大极小的思路的话，总不能一直选最大值吧？

> 第二步是Expansion，就是在前面选中的子节点中走一步创建一个新的子节点，一般策略是随机自行一个操作并且这个操作不能与前面的子节点重复。

这一步实际上就是根据“可选择的行动组”来产生一个子节点。但是怎么保证这个子节点不和前面的子节点重复呢？首先“可选择的行动组”就是当前节点可以走的下一步棋的所有位置的集合，那只要我产生一个子节点的时候，就把这个位置从“可选择的行动组”里边踢出去，那下次又该这个节点模拟的时候，还是从“可选择的行动组”里面找，但是这次找的时候。“可选择的行动组”就已经没有这个位置了。那就是用pop或者remove这种方式就能实现，只要做到“删除并返回”就可以。

> 第三步是Simulation，就是在前面新Expansion出来的节点开始模拟游戏，直到到达游戏结束状态，这样可以收到到这个expansion出来的节点的得分是多少

对于五子棋来讲，这个不是得分，而是谁输谁赢或者平局。模拟游戏也很简单，就是再次通过state类里的get_available_actions方法，得到当前state的所有能下的位置，然后根据下标随机的选一个下标出来，然后下一颗棋，然后把下完这颗子的棋盘和玩家包装成一个新的state，然后外边加一个while循环起来，直到胜负已分或者平局的时候break出来，并记录这个局面最终的情况，就行了

> 第四步是Backpropagation，就是把前面expansion出来的节点得分反馈到前面所有父节点中，更新这些节点的quality value和visit times，方便后面计算UCB值。

这个也很容易实现，就是调用这个函数的肯定是一个节点，那么就可以用一个this来改这个节点的N和Q，改完之后，再让这个节点的父节点也调用反向传播，这样就递归起来了。但是需要加一个停止递归的条件，就是前面说的，当父节点的父节点是null的时候，停止递归。

>![preview](https://pic1.zhimg.com/v2-fa50d56d14f560cba37fdc98be1824e0_r.jpg)

这个图真的是深得我心，之前的时候，感觉这个图说的根本不对。现在回过头看这张图，竟然所有的情况，都给考虑到了。

UCB计算的时候，返回：return -abs(1 - self.current_value)。这是评论区里面的一个，我其实心里一直都是这么想的，但是没有想的这么深入，只是感觉不同玩家的结果应该取负。虽然现在看到这个式子我依然没有什么感觉，但是我需要仔细推敲一下。

### 7.MCTS算法实现

**这一部分我需要批判性的看，以防被带跑偏了。**

由于这是一棵树，我们需要定义N叉树的Node数据结构。

> ![img](https://pic1.zhimg.com/80/v2-54dd9b161ca06c78a585cec6dbb56278_1440w.jpg)
>
> 首先Node包含了parent和children属性，还有就是用于计算UCB值的visit times和quality  value，为了关联游戏状态，我们还需要为每个Node绑定一个State对象。Node需要实现增加节点、删除节点等功能，还有需要提供函数判断子节点的个数和是否有空闲的子节点位置。

增加节点就是通过“可选择的行动组”去增加。删除节点的话，只要没有指针指向节点，应该就能自动删除。不指向，就是在子节点列表里根据行动删掉就行了。判断子节点个数，就是size()方法能实现。是否有空闲的子节点位置，就看“可选择的行动组”里有没有元素。有就是还有空闲位置。没有就是没有空闲位置了。但是一定注意，产生子节点的时候，需要在“可选择的行动组”里删掉这个相应的位置

>而State的定义与我们使用MCTS的游戏定义有关，我们定义下面的数据结构。
>
>![img](https://pic2.zhimg.com/80/v2-423f3be3b05cf46e7dc07135d8e1cb21_1440w.jpg)
>
>每一个State都需要包含当前的游戏得分，可以继续当前游戏玩到第几轮，还有每一轮的选择是什么。当然更重要的，它还需要实现is_terminal()方法判断游戏是否结局，实现compute_reward()方法告诉用户当前得分是多少，还有提供get_next_state()方法用户进行游戏得到新的状态，几个函数与游戏场景游戏，这里简单实现了一个“选择数字保证累加和为1”的游戏。
>
>![preview](https://pic4.zhimg.com/v2-293784fdeb5742db9e095d468c9f791b_r.jpg)

不同游戏的state是不一样的。对于五子棋来说，就是包括棋盘和当前该谁下棋这两个属性。is_terminal()就是检查五子棋是否结束了，这个容易实现。get_next_state()也好弄，就是通过包装当前局面，来产生下一个局面。compute_reward()我感觉跟is_termanal()作用是一样的，可能不同游戏有区别。

> 要实现伪代码提到的几个方法，我们直接定义一个monte_carlo_tree_search()函数，然后依次调用tree_policy()、default_policy()、backup()这些方法实现即可。
>
> ![preview](https://pic4.zhimg.com/v2-06fa5f47ac1142d6d2de9a73e17804df_r.jpg)
> 
> 为了避免程序无限搜索下去，我们需要定义一个computation  budget，限制搜索次数或者搜索时间，这里限制只能向下搜索1000次，然后通过下面的方法来找到expansion  node、计算reward、并且backpropation到所有有关的节点中。

这个算是最好弄的了...就是连起来就行。但是需要注意，先产生一个根节点，然后把根节点带到这个函数里面去。

>![img](https://pic4.zhimg.com/80/v2-9435236175b2a85fa8dd0a03b9727223_1440w.jpg)
>
>这里代码很简洁，实现了一个策略，就是检查如果一个节点下面还有未探索的子节点，那么先expansion下面的子节点就可以了，如果没有子节点，那么就用best_child()函数（其实也就是UCB算法）来得到下一个子节点，然后便利下直到有未探索的节点可以探索。

直接上我个人理解的伪码：

```python
def tree_policy(v):
    while not v.is_terminal():  # 如果节点v是终端节点，直接返回
        # 在不是终端节点的情况下：
        if not v.is_fully_expand():  # 如果发现没有完全探索（
            					     # 没有完全探索的意思是含有子节点的N值为0，或者子节点没有全部产生）
            return v.expand()  # 进行探索
        else:
            v = v.best_child(C_Param)  # 如果已经全部探索，选UCB最大的
    return v
```

>![img](https://pic4.zhimg.com/80/v2-5cb4375d974a09f1b57ed24f1aa5e023_720w.jpg)
>
>best_child()算法很简单，就是根据Node的State获取quality value和visit times，然后计算出UCB值，然后比较返回UCB值最大的子节点而已。

这个依然没有体现出双方的对抗性。

>![img](https://pic3.zhimg.com/80/v2-8ed8f022bbd1c75c273cabd100b90d56_720w.jpg)
>
>expand()函数实现稍微复杂些，实际上就是在当前节点下，选择一个未被执行的Action来执行即可，策略就是随机选，如果有随机选中之前已经执行过的则重新选。

直接上伪代码：

```python
    self.available_actions = self.state.get_available_actions()  # 在当前节点产生的时候初始化一次
    self.children = {}  # 初始化子节点列表
    
def expand(self):
    max_index = len(available_actions)
    # 当挑出来一个action的时候，顺便删掉这个action在列表中的位置
    random_action = self.available_actions.remove(np.random(max_index))
    new_state = self.state.deepcopy()
    new_state.board[random_action] = self.state.player
    sub_node = Node(new_state, self)
    children[action] = sub_node  # 把当前节点和关联的动作添加到字典
    return sub_node
```

>因此tree_policy()方法就是根据是否有未探索子节点和UCB值（也就是权衡和exploration和exploitation）后选出了expansion节点，然后就是用default_policy()来模拟剩下的游戏了。
>
>![img](https://pic4.zhimg.com/80/v2-0c03ddb7af1c622cfb5703989f480f8b_720w.jpg)
>
>在这个游戏种模拟也很简单，我们直接调用State类中实现的随机玩游戏策略，一直玩到最后得到一个reward值即可

直接上代码：

```python
def default_policy(self):
    current_state = self.state.deepcopy()  # 首先需要保证数据安全，将state隔离开
    # is_terminal()方法返回的是当前局面信息：HUMAN_WIN, COMPUTER_WIN, ALL_FILED, NOT_WIN
    whlie current_state.is_terminal() == NOT_WIN:  # 只要没赢，就一直模拟下棋
        available_actions = current_state.get_available_actions()
        max_index = len(available_actions)
        action = available_actions[np.random(max_index)]
        current_state.board[action] = current_state.player
        # next_player = State.get_opponent(current_state.player)
        # current_state = State(current_state.board, next_player)
        # 或者不用新建state，直接改旧的state的值：(可节省内存)
        current_state.player = State.get_opponent(current_state.player)
        winner = current_state.is_terminal()
    return winner
```

> 那么最后我们就需要把前面计算的这个reward反馈到“相关的”的节点上了，这个相关的意思是从根节点开始一直到这个expansion节点经过的所有节点，他们的quality value和visit times都需要更新，backup()函数实现如下。
>
> ![img](https://pic4.zhimg.com/80/v2-4cbc1c775db56ca5c6d33b6741c1d6f7_720w.jpg)

同样直接上伪码：

```python
def backup(self, winner):
        self.N += 1
        if winner != ALL_FILED:
            if winner == self.player:
                self.Q += 1
            else:
                self.Q -= 1
        if self.parent.parent != None:  # 这里判断条件究竟是"父节点的父节点"，还是"父节点"，我还没搞懂
            self.parent.backup(winner)
```



### 8.AlphaGo算法区别 

### 9. 总结

## 三、28 天自制你的 AlphaGo (6) : 蒙特卡洛树搜索（MCTS）基础

原文章：https://zhuanlan.zhihu.com/p/25345778

### 1.极小极大（MiniMax）搜索

> Minimax 搜索的核心思想：**在搜索树中，每次轮到黑棋走时，走对黑棋最有利的；轮到白棋走时，走对黑棋最不利的。**

这一大段里面唯一有用的一句。

### 2.蒙特卡洛树搜索

> C 是一个常数。C 越大就越偏向于广度搜索，C 越小就越偏向于深度搜索。注意对于原始的 UCT 有一个理论最优的 C 值，但由于我们的目标并不是最小化“遗憾”，因此需要根据实际情况调参。

说明了C的作用，并且还让我想到“一种更好的超参数调优方式”那篇文章

> 注意：最终应该选择访问量最大的节点，而不是胜率最高的节点，简单地说是因为访问量最大的节点的结果更可靠

## 四、UCT（信心上限树算法）解四子棋问题——蒙特卡罗法模拟人机博弈

源地址：https://blog.csdn.net/qq_38474694/article/details/96654038?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task

由此文章扩展的文章：https://www.write-bug.com/article/1901.html

> ![img](https://img-blog.csdn.net/20140528192717515?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM5NzcyOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
> 
>解释一下伪代码中出现的符号：s表示状态（state），在四子棋问题中对应某一时刻的棋局信息即双方的棋子是如何排布的，s（v）为状态函数即节点v所对应的状态，s0为初始状态即某一轮到我方落子的状态；v表示节点与状态s一一对应；Δ（delta）表示单次的收益或者说报酬，表征从当前状态按某一策略进行到棋局结束时的胜负情况，而Q（v）表示节点v的综合受益，是经过多次模拟后得到的收益，即很多个不同的Δ的和；N（v）表示节点v被访问的次数，与Q（v）被用于计算胜率及信心上限的估计值；A（s）表示状态s的行动集，即状态s下所有合法的落子方式或者说落子位置的集合，a（v）则表示某一行动方式。

经典老图。

>再解释一下估计信心上限值的算式：c为比例系数（coefficient），控制后一项在整体估计中的重要程度；前一项收益Q比上访问次数N即表示胜率；后一项用于保证，在博弈树规模尚小时，某一节点v不会因为在当前经过模拟累积的收益值较小而不被选择，事实上，适当地选择单次收益Δ的计算方式以及系数c的值，可以使得在博弈树规模尚小时被访问次数越少的节点信心上限值越大，越容易被选择（可以简单手动模拟一下这种情况加深理解），随着节点深度的加深，这一项的影响会越来与小，最终的信心上限值将主要依赖于前一项，胜率。

解释了一下UCB的含义。

> 搜索树策略（Tree  Policy）：所谓终止节点即为棋局结束所对应的状态节点，也就是胜负已分的情况；**可扩展节点指的并不完全是胜负未分的节点，而是针对当前已经构建的这棵博弈树而言的，首先它不是终止节点，其次它有未被访问过的子状态或者说未被扩展的子节点**，这两者还是比较好理解的。

可扩展就是未完全展开。未完全展开就是子节点中还是有没访问过的节点。没访问就是N为0

>回溯（Backup）：从新扩展的节点或者终止节点，逐层向上更新收益和访问次数至根节点。**由于奇偶性不同的层分别对应两方持有棋权的状态，所以单次收益Δ向紧邻的上层传递的时候要取负。**（为避免在实际实现时产生混乱，**可以用收益的正负值分别固定代表两方地胜负情况，然后在Δ向上传递的过程中始终不变号，而在计算信心上限估计值的时候进行判断，获得等价的效果**。

实际上上面那个backup伪码，用的就已经是第二种思路了。

>```c++
>	//最优子节点
>Node *bestChild() {
>    Node* best;
>    double maxProfitRatio = -RAND_MAX;	//初始时，取极小值
>    for (int i = 0; i != column; i ++) {
>        if (children[i] == NULL) continue;
>        double modifiedProfit = (_chessman == PLAYER_CHANCE ? -1 : 1) * children[i] -> profit; //修正收益值
>        int childVisitedNum = children[i] -> visitedNum; //子节点访问数 
>        double tempProfitRatio = modifiedProfit / childVisitedNum + 
>            sqrtl(2 * logl(visitedNum) / childVisitedNum) * VITALITY_COEFFICIENT; //计算综合收益率 
>        if (tempProfitRatio > maxProfitRatio || (tempProfitRatio == maxProfitRatio && rand() % 2 == 0)) { //选择综合收益率最大的子节点 
>            maxProfitRatio = tempProfitRatio;
>            best = children[i];
>        }
>    }
>    return best;
>}
>```

他弄的这个变量太特么恶心了，所以我自己照着写一遍：

```C++
	//最优子节点
Node *bestChild() {
    Node* best;
    double weight = -INFINITY;	//初始时，取极小值
    for (int i = 0; i != column; i ++) {
        if (children[i] == NULL) continue;
        double modifiedProfit = (_chessman == PLAYER_CHANCE ? -1 : 1) * children[i] -> profit; //修正收益值
        int child_N = children[i] -> N; //子节点访问数 
        double current_weight = modifiedProfit / child_N + 
            sqrtl(2 * logl(N) / child_N) * C_Param; //计算综合收益率 
        if (current_weight > weight || (current_weight == weight && rand() % 2 == 0)) { //选择综合收益率最大的子节点 
            weight = current_weight;
            best = children[i];
        }
    }
    return best;
}
```

写完了之后我依然不明白他这个修正收益究竟是为了啥。难道直接取负数不行吗？？？非要弄这么麻烦？

>（2）因为信心上限的估计值可能为负值，所以在选取最大值的时候，max的初值应该设置为一个绝对值很大的负数，比如RAND_MAX什么的。千万不要设置成0就完事了。
>
>（3）在扩展完新的节点开始进行模拟的时候，注意棋权的归属，也就是第一步到底是谁落子，弄反了将会出现的情况就是信心上限估计值大部分情况都是负值，因为对方总是至少比你多一个子。比较正常的情况是，在刚开局的时候使用UCT求解，越靠近中间的可落子位置访问次数越多（趋势比较明显），信心上限值也越大（相对于前者趋势并不明显），可以据此进行相应的调试。

这两条虽然很重要，但是我感觉不出来应该在哪里用。本来我设计的时候就考虑了玩家的情况，如果UCB的时候还考虑，可能会发生意外情况吧。

>额外说明：
>
>（1）首先得把谁是user谁是machine理解清楚：这种称呼方式是针对人机对战的，所以machine指的是我所写的AI，而user也就对应了对手，弄反了的结果可能就是你的AI将以很大的概率为对手助攻；
>
>（2）Node中depth这一描述节点深度的属性是可以被忽略的，最初主要是想用深度作为计算信心上限的一个依据，但测试发现效果并不理想，所以作罢；
>
>（3）对于这种不长不短的程序还是要慢慢写，变量名合理一些，结构清晰一些，不仅减少了错误的出现还利于调试，总之真正在编写上多花一些时间是值得的。

我往上backup的时候，是back的winner，而不是得分，所以一辈子也不可能弄反。 关键就是如何处理UCB，让对手的收益达到最小。

## 五、自制Alpha Go，基于深度增强学习和蒙特卡洛树搜索的五子棋AI（强化学习入门）附代码

原链接：https://zhuanlan.zhihu.com/p/59567014

链接配套的代码：https://github.com/pandezhao/alpha_sigma

拓展出的评论区代码：https://github.com/LeelaChessZero/lczero

### 简介

> 算法分析：强化学习在这里主要由两个部分组成，一个部分是环境(environment），另一个部分是策略（policy）。环境由三个部分组成（状态(state)，动作(action)，奖励（reward））通俗点来讲，环境就是一个黑箱函数，该函数的输出为当前的state和上一个action的reward，而接受的输入为action。用围棋来举例子就是，围棋当前棋盘上的棋子的位置就是状态，而我们选择下了一步棋，那么棋盘的状态就发生了改变（多了一个字），我们之前选择下的那步棋的好坏就是我们的reward。而策略（policy）是在这里抽象为一个输入状态，输出action的函数。policy比较类似人类的思考过程，棋手（policy）通过观察棋盘（state），下了一步棋（做出action）。所以强化学习就可以理解为寻找一个输入状态输出动作，来使得我们的环境反馈的reward最大的一个函数。

让我想到了黑盒优化。

> ，假设我们的棋盘大小为 15 * 15  ，而初始棋盘（棋盘上什么都没有的状态）的状态就是我们的最开始的根节点状态，而其下理论上有225个子节点，分别代表了初始玩家下在225个不同位置时的棋盘状态，而这225个字节点，每个子结点其下理论上又单独可有最多224的子节点，以此类推。对于每个节点，上面储存着该节点的访问次数（counter）与每一个子节点的Q值（每一个子节点相对于父节点来说代表着一个走子的动作{action}，而Q值在这里可以简要理解为该动作好坏的评分）。**PS：在这里我只简要介绍与强化学习相结合的MCTS，不介绍原版的MCTS，普通的蒙特卡罗树与我们这里使用的蒙特卡罗树有区别，请注意。**

原版的mcts确实和五子棋这个不大一样。[b站视频](https://www.bilibili.com/video/BV1p4411U7CK)上讲了一个版本的mcts（评论区中有相应的[实现](https://github.com/ZhiHanZ/monte_carlo_tree_search)），这个mcts相比于五子棋的感觉更传统一些，但是更难实现。这篇文章应该用的就是b站那个版本的思路：把选择这一步完全交给电脑，而不是人为的设定条件。

### 蒙特卡洛树搜索

> 与传统蒙特卡罗树搜索不同，Alpha Zero的模拟是在神经网络输出结果指导下的模拟。传统的MCTS算法如图所示（图片来自于维基百科）。
>
> ![img](https://pic2.zhimg.com/80/v2-5a8300f1141f1cea64ad7c9c48c617d1_720w.jpg)
>
> 模拟过程一般会进行很多次，我们首先对其中一次进行讲解。对于传统的蒙特卡罗树搜索来说，首先基于选取一个“最优”动作（这里的最优要打引号，因为这并不是真的最优，而是当前该动作的一种综合 估计值+置信度 的衡量，具体后面讲UCB公式时我会讲。）

其实最关键的就是这个“最优动作”是怎么选出来的

> 我们持续不断的选取“最优”动作，直到我们来到一个节点，并选择了一个我们之前从来每访问的动作，也因此这里并没有一个与该动作对应的节点。到这里我们完成了图中selection的部分。

根据动作来产生节点。

> 然后进入expansion部分，我们在这里创建新节点，记住，每次模拟只创建一个节点。

到这里目前跟我想的还是比较一致。expand()返回的是一个节点，这个节点应该优先选未探索过的，并且不能和前面选的一致。

> 然后进入MCTS的Simulation部分（注意这里的Simulation与上文中提到的模拟Simulation并不是同一个，做好区分）传统的MCTS会进行随机策略，抽象到棋盘上就相当于在棋盘上面随机走子。

随机走子很好实现。

> 我们就来到了Backpropagation的部分，我们将随机走子的结果（访问次数，胜负之类的信息）更新到这次模拟所经历的所有节点中，也就是说递归更新所有子节点的父节点。

这个也是比较容易实现的。只要注意停止递归的条件就行了

> UCB公式为：
>
> ![[公式]](https://www.zhihu.com/equation?tex=UCB+%3D+Q%28s%2C+a%29+%2B+U%28s%2C+a%29) ，
>
> 其中
>
> ![[公式]](https://www.zhihu.com/equation?tex=U%28s%2C+a%29+%5Csim+c%2A+P%28s%2Ca%29%2A%5Cfrac%7B%5Csqrt%7B%5CSigma_%7Bb%7D+N%28s%2Cb%29%7D%7D%7B%281%2BN%28s%2Ca%29%29%7D) 。
>
> 在这里UCB是一个权衡置信度（或者方差）和探索值的公式，通俗点来说，就是未探索的action和已经探索很多次但是探索反馈很高的action都会有较大的UCB值。Q值代表蒙特卡罗树中以探索的值，Q值的更新就是在之前蒙特卡罗树搜索中每一次backpropagation反馈的值的平均值。而P值是先验概率，是由神经网络计算得到的值，而N是该action的探索次数，而c是一个用于调节平衡模型探索的**超参数**。所以这个公式的特点就是，对于一个反复探索同时有很好的奖励的action，和缺乏探索的action都会有很高的值。这样就能很好的平衡探索与最优之间的平衡了。

超参数的定义：在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。

理解：超参数也是一个参数，是一个未知变量，但是它不同于在训练过程中的参数，它是可以对训练得到的参数有影响的参数，需要训练者人工输入，并作出调整，以便优化训练模型的效果。

### 神经网络

## 六、AlphaZero实战：从零学下五子棋（附代码）

原链接：https://zhuanlan.zhihu.com/p/32089487

配套的程序：https://github.com/junxiaosong/AlphaZero_Gomoku

扩展出的程序：https://github.com/zouyih/AlphaZero_Gomoku-tensorflow

文章看不懂，但是评论区里有一些让我注意到了：

> **Q：**感谢博主分享知识各源码。我对代码有一些疑问请教。我看其他关于mcts的文档。其中Q是指 Total simulation reward , N是 Total number of visits 。 UCT是 `Q/N + c *  sqrt( log(Nv)/ Nvi)`,   可是从您的代码来看, 其中 TreeNode.update中, 
>
> `self._Q += 1.0 * (leaf_value -  self._Q) / self._nvisits` 其中`self._Q`是 `leaf_value-Q`再除以`self._n_visits`,  是一个小于1的数，而且与`leaf_value`再加减. 而且`leaf_value`似乎应该是1 或者  -1或者0  ，我的理解似乎应该是`self._Q`是胜负的次数，而不是一个百分比. 
>
> 在`TreeNode.get_value()`中,  
>
> `self._u = c_puct * self._P * np.sqrt(self._parent._n_visits) / (1 + self._n_visits)`
>
> 似乎没有对`self.parent.n_visits`取`log`.. 这个是什么原因阿？
>
> **A：**首先第一个，`self._Q`是类似于胜率的，不是获胜的次数，直接对应于你贴的`UCT`公式中的`Q/N`吧，是把`N`已经除进去了；第二点，你贴的是原始的`UCT`公式，`UCT`有很多变体的，代码里实现的是AlphaGo Zero论文里用的`puct`的公式，分子没有取`log`，分母没有开根号
>
> **Q：**而且在调用递归更新函数的时候，把`-leaf_value`胜负调换，我看不明白
>
> `\# Update value and visit count of nodes in this traversal.`
>
> ​        `node.update_recursive(-leaf_value)`
>
> **Q：**您好，问一个编程性问题😀 在函数update_recursive()里，递归调用的是
>
> `if self._parent:`
> `self._parent.update_recursive(-leaf_value)`
>
> `-leaf_value`为什么要有负号呢
>
> **Q：**再请问一下，c_puct的值是自己算出来的还是官方论文给定参数，temp值是否需要在训练过程中随步数进行退火，打扰您了，望回复一下
>
> **A：**c_puct是一个参数，控制MCTS探索与利用的平衡，代码里的值是我自己设置的；temp值在alphazero论文中是每局对弈前30步取1，后面取0，但我这边一直取的1，没有衰减

## 七、Alpha Zero版五子棋实现

原链接：https://ne7ermore.github.io/post/alpha-zero/

配套的代码：https://github.com/ne7ermore/torch-light/tree/master/alpha-zero

### 阅读和实现论文笔记

### 细节

> 在mcts中，每次更新父节点时注意value的正负值

### 代码解释

在代码中，反向传播的是对于一个局面的奖励。这个奖励在向上传播的过程中取了负数